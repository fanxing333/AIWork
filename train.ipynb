{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2501b98e-ce47-410b-bb16-c79a68632c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.SimpleCNN import SimpleCNN\n",
    "from models.AlexNet import AlexNet\n",
    "from models.ResNet import ResNet, ResidualBlock\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c7913096-ae9c-4968-963e-c7932e5554a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "an = AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "717bc4c4-7c13-44c1-89e8-4d0a8f3e3b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223.69860076904297"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3*11*11 + 11 + 64*5*5 + 64 + 192*3*3 + 192 + 384*3*3 + 384 \\\n",
    " + 256*3*3 + 9216*4096 + 4096 + 4096*4096 + 4096 + 4096*1000 + 1000)*4/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1e7936ea-2c9d-4e0b-bbf2-6a29038126a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223.625"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9216*4096 + 4096*4096 + 4096*1000)*4/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66e00410-51c5-4340-baae-7526153c8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "275a9906-22f5-437b-bd8c-6d6aeaa6d278",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[-0.0333,  0.0651, -0.0805, -0.0726, -0.0010],\n",
       "                        [-0.0628, -0.1703,  0.1084, -0.0266,  0.0225],\n",
       "                        [-0.0297,  0.0236,  0.1508,  0.0582,  0.0581],\n",
       "                        [ 0.1026,  0.1341,  0.0807,  0.1285,  0.1105],\n",
       "                        [-0.1772,  0.1706,  0.1381,  0.1293,  0.0070]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0370,  0.1529,  0.0437,  0.0769, -0.0402],\n",
       "                        [-0.1830, -0.1313,  0.1884, -0.0552,  0.0977],\n",
       "                        [-0.1966,  0.0685, -0.1407, -0.1272, -0.0549],\n",
       "                        [-0.0218,  0.1325, -0.1874,  0.0541, -0.1716],\n",
       "                        [-0.1121, -0.0900, -0.1455, -0.0328, -0.0427]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1476,  0.0029, -0.1518,  0.0579, -0.0667],\n",
       "                        [ 0.0224,  0.1465,  0.1510, -0.1387, -0.0958],\n",
       "                        [ 0.0939, -0.0736,  0.1566, -0.1097,  0.0024],\n",
       "                        [-0.1572, -0.1649, -0.1314,  0.1645, -0.0210],\n",
       "                        [-0.0298, -0.1167, -0.1056,  0.1011, -0.1954]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0453,  0.0064, -0.1390,  0.1950,  0.1454],\n",
       "                        [-0.1826, -0.1646,  0.0784, -0.1364, -0.1568],\n",
       "                        [-0.0253, -0.1463,  0.1325, -0.1834, -0.0823],\n",
       "                        [-0.1256,  0.0544,  0.0322, -0.0752,  0.1992],\n",
       "                        [ 0.0649,  0.0701, -0.1134, -0.0317, -0.0162]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0486, -0.1440,  0.0940, -0.0674,  0.0256],\n",
       "                        [-0.1099,  0.0252,  0.1932, -0.0849, -0.0437],\n",
       "                        [ 0.1399,  0.0705,  0.1881,  0.0942, -0.1741],\n",
       "                        [ 0.1592,  0.0360, -0.0441, -0.0882,  0.1692],\n",
       "                        [ 0.0639, -0.0983, -0.1971,  0.1018,  0.1907]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1046, -0.1944,  0.0744, -0.0369,  0.1084],\n",
       "                        [-0.1616,  0.1197,  0.1039, -0.0384,  0.0147],\n",
       "                        [-0.0351, -0.1806,  0.1749,  0.0280,  0.1320],\n",
       "                        [-0.0782, -0.0353, -0.0605,  0.1054, -0.1182],\n",
       "                        [-0.1939, -0.1617,  0.1563, -0.1904, -0.0451]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.0277, -0.0103, -0.1441, -0.1030,  0.1159,  0.1632])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[ 5.7766e-03, -2.9384e-03,  4.7209e-02, -6.2909e-02,  5.3521e-02],\n",
       "                        [ 6.6250e-02,  4.1031e-02, -9.8664e-03,  5.7328e-02, -1.5129e-02],\n",
       "                        [-8.0435e-03,  5.3763e-02, -3.5250e-02,  5.8786e-02, -2.4325e-02],\n",
       "                        [-2.3993e-02,  7.9023e-02,  3.4168e-02, -4.4801e-02, -5.4583e-03],\n",
       "                        [ 1.5560e-02, -4.3738e-02, -1.6812e-02, -1.1509e-02,  8.0488e-02]],\n",
       "              \n",
       "                       [[ 6.1576e-02,  4.6919e-02, -1.0328e-02, -4.4918e-02,  6.9484e-02],\n",
       "                        [ 6.2934e-02, -6.0404e-02,  2.1952e-02,  2.5409e-02,  4.6363e-02],\n",
       "                        [ 5.5709e-02,  4.3056e-02,  7.6741e-02, -7.3456e-02, -5.0794e-02],\n",
       "                        [-7.9134e-02, -6.0082e-02,  4.5247e-02,  3.7914e-02,  6.9457e-02],\n",
       "                        [ 3.3668e-02,  3.1144e-03, -7.4097e-02,  7.7248e-02, -3.8755e-02]],\n",
       "              \n",
       "                       [[-1.3088e-02,  7.8083e-02, -3.4077e-02,  4.9131e-02, -5.3453e-02],\n",
       "                        [-8.9727e-03,  6.2489e-02, -5.3044e-02,  2.1322e-02, -4.4830e-02],\n",
       "                        [ 4.5535e-02, -4.6423e-02,  3.1207e-03, -2.9854e-03, -6.7183e-02],\n",
       "                        [ 6.7368e-02, -4.3282e-02, -1.4430e-02, -2.2508e-02, -7.2419e-02],\n",
       "                        [-8.1400e-02, -5.8951e-03,  6.2666e-03,  6.6187e-02, -3.7372e-02]],\n",
       "              \n",
       "                       [[ 4.5949e-03, -4.0804e-03, -1.4916e-02,  7.7328e-02,  6.0525e-02],\n",
       "                        [-8.0099e-02, -3.0400e-02, -5.7490e-02, -5.9405e-02, -4.8788e-02],\n",
       "                        [-3.0763e-02, -5.2582e-02, -1.9148e-02,  2.1313e-02, -2.6726e-02],\n",
       "                        [ 8.1182e-02,  3.2291e-02,  7.9765e-02,  7.4989e-02, -7.8485e-02],\n",
       "                        [ 4.8167e-02,  2.6034e-02,  7.9688e-02,  3.9555e-02, -4.8930e-02]],\n",
       "              \n",
       "                       [[-5.1304e-02, -6.2058e-02, -4.5868e-02, -5.6347e-03, -7.2347e-02],\n",
       "                        [-6.9852e-02, -2.8080e-02,  6.1196e-02,  7.5198e-02, -3.3534e-02],\n",
       "                        [-3.7165e-02,  7.4518e-02,  3.5702e-02, -4.8858e-02,  4.9798e-02],\n",
       "                        [-3.9065e-02,  2.7436e-03, -6.5498e-02,  1.4760e-02, -6.9857e-02],\n",
       "                        [ 3.3481e-02, -2.6643e-02,  1.4143e-02,  2.7972e-02, -2.2492e-02]],\n",
       "              \n",
       "                       [[ 7.3129e-02,  8.0425e-02,  5.1966e-02,  1.9100e-02,  8.0075e-02],\n",
       "                        [-2.7488e-02, -3.9978e-02, -5.1251e-02,  1.2661e-02,  3.8019e-03],\n",
       "                        [-5.6362e-02, -3.2585e-02,  7.7606e-02,  5.4206e-02, -5.8170e-02],\n",
       "                        [ 7.3488e-02, -7.4496e-02, -7.6128e-02,  1.5718e-02,  5.7251e-02],\n",
       "                        [-4.6298e-02,  5.2482e-02, -2.2218e-02,  3.7051e-02, -2.5532e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8356e-02,  4.9603e-02, -1.5793e-02,  6.4677e-02, -8.1061e-02],\n",
       "                        [ 3.1178e-02, -7.0175e-02, -3.7733e-02,  3.5784e-02, -5.1755e-02],\n",
       "                        [-1.0044e-02,  7.9739e-02, -1.0405e-03,  8.0408e-02, -7.2791e-02],\n",
       "                        [ 1.5763e-02, -2.5467e-02,  6.1798e-02,  7.1549e-02, -3.7173e-02],\n",
       "                        [-5.2296e-02, -5.1836e-02, -6.3294e-02,  4.8496e-02,  2.0985e-02]],\n",
       "              \n",
       "                       [[-6.4282e-02, -4.8172e-02,  4.1079e-02, -2.4612e-02, -5.0113e-02],\n",
       "                        [ 1.1783e-02, -2.4181e-02,  7.0573e-02,  2.2390e-02, -3.5183e-02],\n",
       "                        [-1.9784e-02,  4.4389e-02,  2.8297e-02,  3.8135e-02,  3.8768e-02],\n",
       "                        [ 7.3546e-02,  7.4813e-03, -2.2963e-02, -1.2214e-03,  6.2185e-02],\n",
       "                        [-4.7334e-03,  2.5296e-02, -7.1952e-02,  7.4541e-02, -1.5378e-02]],\n",
       "              \n",
       "                       [[-6.1033e-02,  2.7209e-02,  3.2990e-02, -2.3180e-02,  5.1131e-03],\n",
       "                        [ 3.6543e-02,  6.2810e-02, -5.9419e-03, -2.1523e-03, -8.1453e-02],\n",
       "                        [-1.6924e-02,  4.3199e-02, -6.0271e-02,  5.0314e-02,  1.3757e-02],\n",
       "                        [-6.5344e-02,  5.4009e-02, -5.1653e-02,  4.6611e-02, -1.6775e-02],\n",
       "                        [-6.5217e-03, -1.9058e-02, -3.9176e-02, -5.2777e-02, -7.3536e-03]],\n",
       "              \n",
       "                       [[-3.2416e-02,  3.1342e-02, -6.4336e-02, -3.0394e-02, -2.7367e-02],\n",
       "                        [-1.8084e-02, -7.0168e-02,  3.5627e-02, -2.6066e-02, -2.5099e-02],\n",
       "                        [ 1.6323e-02,  6.2320e-02, -2.7457e-02,  3.1524e-03, -4.9265e-02],\n",
       "                        [ 1.5691e-02, -2.5641e-02,  7.8541e-02,  8.1490e-02,  9.1864e-03],\n",
       "                        [ 2.2200e-02, -2.5625e-02,  5.6020e-02,  2.7023e-02,  4.6709e-02]],\n",
       "              \n",
       "                       [[ 3.3922e-02,  5.0301e-02,  5.1944e-02,  2.8670e-02, -7.5447e-02],\n",
       "                        [ 2.4174e-02,  7.0922e-02, -5.1642e-02, -2.1883e-02,  1.9629e-02],\n",
       "                        [-3.1902e-02, -6.7639e-02, -1.4090e-02,  3.6447e-02, -5.5057e-02],\n",
       "                        [-1.3174e-02,  1.9636e-03, -5.1190e-03,  3.2096e-02,  2.5343e-02],\n",
       "                        [-7.0689e-02,  8.0303e-02, -3.3577e-02,  2.7478e-02,  4.2622e-02]],\n",
       "              \n",
       "                       [[-6.7350e-02,  1.5315e-02,  2.2746e-02,  6.6685e-02, -2.1696e-02],\n",
       "                        [-6.6864e-02,  1.0370e-02,  1.3793e-02,  6.6848e-02, -8.0147e-02],\n",
       "                        [-2.4062e-02, -2.1295e-02, -1.0699e-02, -1.9944e-02, -4.0406e-02],\n",
       "                        [ 1.9278e-02, -7.0178e-03, -5.3497e-02, -8.4741e-03, -2.4339e-02],\n",
       "                        [-1.7462e-02,  6.6269e-02, -3.0152e-02, -4.1191e-02,  6.6917e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.8726e-02, -1.0518e-02,  6.8811e-02,  1.6666e-02,  3.3537e-02],\n",
       "                        [ 3.0567e-02,  5.4070e-02, -5.0433e-02, -6.0158e-02,  5.4820e-02],\n",
       "                        [ 5.8157e-02,  4.5305e-02, -4.4747e-02, -4.8707e-02,  3.6129e-02],\n",
       "                        [-1.9067e-02, -1.9297e-02, -5.5452e-02, -7.8801e-02, -4.6229e-02],\n",
       "                        [-3.8722e-02, -7.4024e-02,  7.0657e-02,  1.6880e-02,  7.4132e-02]],\n",
       "              \n",
       "                       [[-1.7970e-02, -1.4565e-02,  5.0515e-02, -1.4335e-05, -3.7612e-02],\n",
       "                        [ 1.5080e-02, -5.0316e-02,  7.7481e-02, -4.3415e-02, -4.9656e-02],\n",
       "                        [-7.4435e-02,  8.0501e-02, -7.1766e-02,  7.0921e-02,  2.9802e-02],\n",
       "                        [-7.4239e-02,  1.2304e-04,  4.5844e-02, -5.2367e-02, -1.8011e-02],\n",
       "                        [ 5.6172e-02, -2.8565e-03,  2.7779e-02, -4.6229e-03,  4.4023e-02]],\n",
       "              \n",
       "                       [[ 4.4460e-02,  6.5260e-02,  1.5754e-02, -1.5613e-02, -5.2522e-02],\n",
       "                        [ 2.8905e-02,  8.9911e-03, -2.4227e-02,  4.1871e-02, -1.5850e-02],\n",
       "                        [-5.4324e-02, -6.3043e-04,  3.8614e-02,  3.5015e-03, -4.2785e-02],\n",
       "                        [-5.2699e-03, -2.5672e-02, -2.9566e-03, -2.5248e-02,  3.6207e-02],\n",
       "                        [-4.0478e-02, -6.8581e-02, -1.5314e-02,  5.7462e-02, -3.7562e-02]],\n",
       "              \n",
       "                       [[-1.8878e-02,  3.8372e-02,  5.1683e-02,  8.0688e-02, -3.7466e-02],\n",
       "                        [ 3.2787e-02,  1.9232e-02, -2.8074e-02, -3.5126e-02, -3.6310e-02],\n",
       "                        [ 4.9000e-02, -5.3971e-02, -7.2324e-02,  6.8346e-02, -7.3781e-02],\n",
       "                        [-6.1538e-02, -5.2812e-02,  7.5673e-02,  5.0687e-02,  8.0394e-02],\n",
       "                        [ 2.9858e-02, -3.3667e-02,  3.0124e-02,  4.6561e-02,  5.3584e-02]],\n",
       "              \n",
       "                       [[ 5.6896e-02,  7.8716e-02, -4.4071e-02, -1.9780e-02, -7.9433e-02],\n",
       "                        [-5.4900e-02, -5.4160e-02,  4.2206e-02,  6.8686e-03, -3.2783e-02],\n",
       "                        [ 7.7704e-02, -4.1596e-02, -4.8221e-02,  1.5675e-02, -1.7733e-02],\n",
       "                        [ 2.5903e-04, -7.3810e-02,  5.7537e-02,  7.4045e-02,  7.8685e-02],\n",
       "                        [ 8.1547e-02, -2.3725e-02, -3.4899e-02, -6.6914e-02,  1.6949e-03]],\n",
       "              \n",
       "                       [[ 2.7249e-02,  1.5092e-02, -2.8872e-02, -5.4651e-02, -4.7843e-02],\n",
       "                        [-8.0680e-02, -2.8733e-02,  5.4088e-03, -1.0851e-02, -4.8161e-02],\n",
       "                        [ 1.7673e-02,  3.3385e-02, -2.9153e-02,  2.1166e-02,  2.2418e-02],\n",
       "                        [-7.3509e-02, -7.6897e-02, -6.9932e-02,  6.1309e-02, -4.2500e-02],\n",
       "                        [-3.6843e-02, -1.5254e-02, -1.5219e-02,  4.0340e-02, -6.3833e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 7.8914e-03,  1.9556e-02, -1.5522e-02,  2.4912e-02, -7.3131e-03],\n",
       "                        [-7.0394e-04, -3.1225e-02,  1.6855e-03,  2.7943e-04,  4.0963e-02],\n",
       "                        [-3.5581e-02, -2.3116e-02, -4.9516e-02,  3.2490e-02, -5.5925e-02],\n",
       "                        [ 2.2958e-02,  4.3119e-02, -4.3519e-02, -6.1292e-02, -9.3823e-04],\n",
       "                        [-7.7868e-02, -4.0578e-02,  7.4203e-02,  5.1714e-02,  1.8178e-03]],\n",
       "              \n",
       "                       [[-4.3190e-02, -2.4831e-02,  6.4104e-02,  8.5769e-03,  5.5384e-02],\n",
       "                        [-1.3621e-02, -8.0451e-02,  6.5867e-02,  5.7773e-02,  6.5379e-02],\n",
       "                        [ 3.8653e-03, -2.1425e-02, -2.0457e-02,  7.1046e-02,  7.6348e-02],\n",
       "                        [-1.1345e-02,  6.2737e-03, -2.2529e-02, -6.0522e-03, -7.7318e-02],\n",
       "                        [-6.6287e-02,  7.8253e-02, -9.3298e-03,  7.8030e-02, -4.8318e-02]],\n",
       "              \n",
       "                       [[-5.0082e-02,  1.4871e-03,  3.6027e-02, -1.1001e-02, -5.9853e-02],\n",
       "                        [ 4.1603e-04, -6.4443e-02, -2.9615e-02,  7.6437e-02,  7.5992e-02],\n",
       "                        [-6.0758e-02,  5.4456e-02, -7.2071e-02, -7.3595e-02, -7.0362e-02],\n",
       "                        [-3.2339e-02,  7.6253e-03, -4.5254e-02, -1.1770e-04, -2.1925e-02],\n",
       "                        [ 6.0392e-02, -7.3036e-02,  3.0256e-02, -7.2476e-02,  1.5735e-03]],\n",
       "              \n",
       "                       [[-3.9223e-02,  7.2459e-02, -1.4802e-02,  1.3512e-03, -5.8200e-02],\n",
       "                        [-4.7296e-02, -4.9998e-03, -5.3901e-02, -2.1376e-02, -4.9421e-03],\n",
       "                        [-3.2201e-02,  7.5301e-02,  2.3450e-03, -1.1660e-02,  3.6441e-02],\n",
       "                        [ 4.0098e-02, -7.3733e-02, -5.5131e-02, -8.8018e-03,  2.8052e-02],\n",
       "                        [-3.8557e-02, -2.0592e-02,  2.7932e-02, -1.7290e-02, -7.0348e-02]],\n",
       "              \n",
       "                       [[ 6.5414e-02, -2.5628e-02,  4.0128e-02,  8.1488e-02, -4.4079e-02],\n",
       "                        [-3.6720e-02, -5.9189e-02,  7.4381e-03, -1.8533e-02, -4.2524e-02],\n",
       "                        [-3.2304e-03,  3.5480e-03,  7.4897e-02,  7.7854e-03,  1.0490e-02],\n",
       "                        [-2.2883e-02,  1.7015e-02, -5.1365e-02,  3.8135e-02, -7.1183e-02],\n",
       "                        [ 3.2895e-03,  1.0966e-02,  5.4680e-02,  3.2067e-02,  2.4223e-02]],\n",
       "              \n",
       "                       [[-2.9022e-02, -7.7560e-02,  6.1206e-02,  1.9503e-02,  2.0581e-02],\n",
       "                        [ 3.2835e-02,  7.8109e-03, -1.0184e-02,  7.0512e-02,  5.9484e-02],\n",
       "                        [ 4.3647e-02, -1.8175e-02,  6.2648e-02,  2.7429e-02, -3.3409e-02],\n",
       "                        [-7.8418e-02, -8.4134e-03,  3.4026e-02, -6.1437e-02, -2.6456e-03],\n",
       "                        [ 6.2000e-02,  6.7223e-02, -4.8032e-02, -1.4091e-02,  3.0570e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.7916e-02,  6.4201e-02,  2.1920e-02,  1.4717e-02,  6.0974e-02],\n",
       "                        [-7.2735e-02, -7.9555e-02,  1.3580e-02,  5.1652e-02, -2.9176e-02],\n",
       "                        [ 6.1915e-02,  3.4724e-02, -2.5611e-02, -5.0314e-02, -5.2393e-02],\n",
       "                        [-1.7465e-02, -8.5491e-03, -6.2374e-02,  3.0475e-02,  7.8822e-02],\n",
       "                        [-7.9659e-02, -7.0755e-02,  7.3335e-02,  6.8664e-02,  8.1533e-02]],\n",
       "              \n",
       "                       [[-9.6918e-03,  9.7064e-03, -6.1307e-02,  5.9158e-02,  7.4048e-02],\n",
       "                        [-7.1833e-03,  1.1339e-03, -5.6848e-02, -1.8193e-02, -2.6185e-02],\n",
       "                        [-3.2246e-02, -6.0445e-02, -2.7459e-02,  3.9615e-02, -6.0237e-03],\n",
       "                        [ 4.3014e-02,  7.2191e-02, -8.0659e-02,  2.5428e-02,  7.8719e-02],\n",
       "                        [-4.1709e-02,  4.7844e-02,  2.1521e-02,  2.1218e-02, -6.8290e-02]],\n",
       "              \n",
       "                       [[ 5.9539e-03, -2.8459e-02,  7.7922e-02, -5.1393e-02, -1.0541e-02],\n",
       "                        [ 5.0417e-03,  3.5777e-02, -5.3119e-02, -8.0847e-02, -1.1089e-02],\n",
       "                        [ 2.0900e-02, -3.2045e-02,  5.5546e-02, -7.1644e-02, -6.0819e-02],\n",
       "                        [-3.2255e-02, -7.6440e-03, -5.5062e-02, -3.7099e-02,  5.7681e-02],\n",
       "                        [-4.6735e-02,  2.2712e-02,  5.7977e-02, -1.2552e-02,  6.3680e-02]],\n",
       "              \n",
       "                       [[ 3.9207e-02, -1.9372e-02, -5.8608e-02,  1.9478e-02, -7.2107e-02],\n",
       "                        [-1.9932e-02,  1.2707e-02,  1.6286e-03, -1.1186e-02, -5.6416e-02],\n",
       "                        [-7.0354e-03,  1.2445e-02,  2.5975e-02,  6.3049e-02, -4.4130e-02],\n",
       "                        [ 8.3326e-04,  4.8073e-02,  5.5891e-02, -3.7192e-02, -2.4102e-03],\n",
       "                        [-1.3453e-02, -4.1272e-02, -1.1709e-02,  7.5598e-02,  7.6190e-02]],\n",
       "              \n",
       "                       [[-7.1487e-02,  7.2453e-02,  5.4003e-02,  5.7815e-02,  1.9353e-02],\n",
       "                        [ 1.7859e-03,  5.8262e-02, -4.2805e-02, -3.2610e-02, -4.0556e-02],\n",
       "                        [-8.0996e-02,  6.2785e-02,  7.9474e-02, -3.3248e-02,  5.5107e-02],\n",
       "                        [ 3.8920e-02, -1.6700e-02, -6.6461e-02, -3.0001e-02,  3.8131e-02],\n",
       "                        [ 1.4461e-02,  2.5053e-02, -6.0161e-02, -3.5637e-02,  3.3748e-02]],\n",
       "              \n",
       "                       [[ 4.8617e-04, -7.4690e-02, -9.1913e-04, -1.1192e-03, -6.1382e-02],\n",
       "                        [-7.6846e-02,  3.2220e-02,  2.2410e-02, -1.0958e-02, -1.6098e-02],\n",
       "                        [-4.4750e-02,  5.0504e-02, -2.0496e-02, -5.4450e-03,  1.0407e-02],\n",
       "                        [-4.9752e-02, -2.7436e-03,  2.3350e-02,  7.1861e-03,  5.0491e-02],\n",
       "                        [-2.9401e-02, -2.5376e-02, -5.1401e-02, -2.9482e-02,  6.8493e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2529e-02,  4.6980e-02,  6.6245e-02, -5.5660e-02, -6.4194e-02],\n",
       "                        [ 6.6246e-03,  7.8111e-02,  4.7123e-02,  4.8630e-02,  7.3609e-02],\n",
       "                        [ 2.4580e-02,  6.3371e-02,  1.3765e-02, -4.2806e-02, -4.5990e-02],\n",
       "                        [-1.0858e-02, -5.7255e-02,  5.8810e-02,  8.7373e-03, -6.9552e-02],\n",
       "                        [-3.5941e-02,  2.4246e-02, -3.3219e-02, -5.4067e-03, -1.0191e-02]],\n",
       "              \n",
       "                       [[-5.0021e-02, -6.9663e-02, -3.5510e-02, -2.5899e-02,  2.1842e-02],\n",
       "                        [ 6.3476e-02,  7.2723e-02,  3.2751e-02,  3.1185e-02,  5.0772e-02],\n",
       "                        [-1.9373e-02,  2.8426e-02, -6.2379e-02, -3.4010e-02, -2.5220e-02],\n",
       "                        [-2.4670e-03,  1.2913e-02, -6.2947e-02,  2.3873e-02,  4.4708e-03],\n",
       "                        [ 4.8258e-02, -8.9554e-03,  4.2688e-02, -1.6604e-02,  7.0376e-02]],\n",
       "              \n",
       "                       [[-2.4357e-02,  6.5753e-02, -7.2410e-02, -2.1075e-03, -2.9873e-03],\n",
       "                        [-3.4312e-02, -6.5599e-02, -6.0823e-02, -3.1724e-02, -5.1970e-02],\n",
       "                        [ 3.6458e-02, -4.9822e-02,  7.8575e-02, -4.0034e-02,  9.3004e-03],\n",
       "                        [ 2.0891e-02, -2.2270e-02,  8.6195e-04,  7.1207e-02, -2.4785e-02],\n",
       "                        [-3.3854e-02, -7.2113e-02, -2.3355e-02, -2.0865e-02, -5.6502e-02]],\n",
       "              \n",
       "                       [[-1.8722e-02, -8.1825e-03,  2.6166e-02, -4.9598e-02,  7.8808e-02],\n",
       "                        [ 1.5173e-02,  1.4579e-02, -3.8847e-02,  4.4790e-02, -1.7693e-02],\n",
       "                        [ 7.8417e-03, -5.6361e-02, -4.6963e-02,  6.0298e-02,  3.4006e-02],\n",
       "                        [ 5.5793e-02, -7.0150e-02, -5.7715e-02, -4.2515e-02, -3.1972e-02],\n",
       "                        [ 7.2450e-02, -5.0013e-02, -6.2241e-04, -6.4883e-02, -1.9392e-03]],\n",
       "              \n",
       "                       [[-7.5818e-02, -6.6928e-02,  7.6197e-02,  5.1421e-02,  3.2742e-03],\n",
       "                        [-5.3056e-02,  6.3727e-02,  6.2334e-02,  4.5683e-02, -4.9085e-02],\n",
       "                        [-3.8016e-02,  3.3493e-02,  6.5296e-03, -5.8315e-02,  2.0638e-03],\n",
       "                        [ 3.1226e-03, -6.4005e-02,  2.3283e-02,  7.4473e-02, -4.0653e-02],\n",
       "                        [-4.1100e-02,  1.2576e-02, -1.3463e-02,  1.2647e-02,  4.8806e-02]],\n",
       "              \n",
       "                       [[-1.4312e-02, -4.5024e-02, -3.8605e-02, -3.8037e-02,  5.7453e-02],\n",
       "                        [ 2.8090e-02,  3.3294e-02, -1.2263e-02,  3.4506e-02,  3.1955e-03],\n",
       "                        [-5.9121e-02, -2.8662e-02, -1.5904e-02, -6.9885e-02, -3.7930e-02],\n",
       "                        [-4.1526e-02, -4.8358e-02,  3.5333e-02, -1.6368e-03,  4.2265e-03],\n",
       "                        [-7.0655e-02, -2.7663e-02,  5.9752e-02,  5.5727e-02,  4.9221e-02]]]])),\n",
       "             ('conv2.bias',\n",
       "              tensor([-0.0442,  0.0459,  0.0550, -0.0567, -0.0565, -0.0653, -0.0065, -0.0122,\n",
       "                       0.0244,  0.0562, -0.0158,  0.0731,  0.0455,  0.0250, -0.0075,  0.0669])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0022,  0.0487,  0.0480,  ..., -0.0438,  0.0128, -0.0013],\n",
       "                      [-0.0348, -0.0222, -0.0053,  ..., -0.0243,  0.0479, -0.0437],\n",
       "                      [-0.0115,  0.0325,  0.0116,  ..., -0.0260,  0.0203,  0.0117],\n",
       "                      ...,\n",
       "                      [-0.0128, -0.0323, -0.0113,  ..., -0.0043, -0.0038,  0.0146],\n",
       "                      [-0.0309,  0.0309, -0.0446,  ...,  0.0311,  0.0379,  0.0171],\n",
       "                      [ 0.0398,  0.0041,  0.0422,  ...,  0.0469, -0.0221,  0.0448]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 3.9515e-03,  1.7562e-02,  3.1346e-02,  3.7023e-02, -4.0844e-02,\n",
       "                      -2.1192e-02,  2.1032e-02, -2.2750e-02,  3.8179e-02, -3.1793e-02,\n",
       "                       1.7930e-02, -7.9961e-03,  3.2004e-02,  1.4010e-02,  2.1969e-02,\n",
       "                       8.4293e-03, -2.8614e-04, -1.5258e-02, -3.7236e-02,  2.8957e-02,\n",
       "                      -2.9538e-02, -1.2032e-02,  3.9042e-03, -1.6309e-02,  4.4480e-02,\n",
       "                       3.7244e-03,  3.9778e-02,  4.9809e-03, -2.4723e-02, -4.0183e-02,\n",
       "                      -2.2952e-02,  2.6018e-03, -4.0562e-02,  1.7574e-02,  4.0643e-02,\n",
       "                      -1.6276e-02, -3.8196e-02,  4.9864e-02,  4.8920e-02,  3.1957e-02,\n",
       "                       2.4384e-02,  2.2866e-02, -3.1767e-02, -9.6450e-03,  1.5259e-02,\n",
       "                      -1.9269e-02, -1.4023e-02, -3.0696e-05, -6.1290e-03, -5.1168e-03,\n",
       "                      -1.2028e-02, -4.4019e-03,  2.3110e-02,  9.5558e-03, -4.4224e-03,\n",
       "                       5.8867e-03,  2.0858e-02, -1.2960e-02, -1.5906e-03, -4.6888e-02,\n",
       "                      -2.2247e-02,  2.0885e-02,  1.5459e-02,  8.3434e-03, -3.6679e-02,\n",
       "                       1.2075e-02, -3.6590e-02,  6.6847e-03, -1.1769e-02, -3.6171e-02,\n",
       "                       3.8684e-02, -4.6338e-02,  4.7208e-02,  3.6252e-02,  5.1739e-03,\n",
       "                      -4.8761e-02, -4.0696e-02, -3.1906e-02,  2.3557e-02,  3.5741e-02,\n",
       "                       2.2975e-02, -3.6990e-02,  3.9840e-02, -3.9608e-02,  3.5349e-02,\n",
       "                      -4.7320e-02,  1.1058e-02,  4.3715e-02,  2.2911e-02,  1.5369e-02,\n",
       "                      -2.2354e-02,  9.6502e-03,  2.6703e-02,  3.3893e-02,  1.3335e-02,\n",
       "                      -1.7721e-02, -1.1853e-02, -8.0325e-03, -4.8552e-02, -2.7985e-02,\n",
       "                       4.5859e-02, -9.6239e-04, -4.4480e-02, -7.2797e-04,  3.3727e-03,\n",
       "                      -2.7833e-02, -5.9146e-03,  3.5574e-02, -3.6761e-02, -4.1426e-02,\n",
       "                       2.7737e-02, -2.3211e-02,  1.0392e-02,  3.6289e-02,  4.0554e-03,\n",
       "                       4.8948e-02, -3.7815e-02,  4.1877e-02, -4.6758e-03, -2.5758e-02])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0481,  0.0186, -0.0409,  ..., -0.0569, -0.0761, -0.0879],\n",
       "                      [ 0.0285, -0.0326, -0.0636,  ...,  0.0122, -0.0190,  0.0271],\n",
       "                      [ 0.0830,  0.0123, -0.0453,  ...,  0.0435,  0.0888, -0.0586],\n",
       "                      ...,\n",
       "                      [ 0.0126, -0.0885,  0.0246,  ..., -0.0695,  0.0751,  0.0734],\n",
       "                      [-0.0173, -0.0464, -0.0060,  ..., -0.0566,  0.0676, -0.0121],\n",
       "                      [-0.0800,  0.0446,  0.0801,  ..., -0.0557, -0.0599, -0.0526]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0553, -0.0876,  0.0474, -0.0364,  0.0508,  0.0782, -0.0669, -0.0786,\n",
       "                       0.0295,  0.0738,  0.0136,  0.0526,  0.0793,  0.0059,  0.0018, -0.0480,\n",
       "                      -0.0769,  0.0063,  0.0517, -0.0417, -0.0670, -0.0567,  0.0561,  0.0844,\n",
       "                       0.0457,  0.0143,  0.0871, -0.0288,  0.0040, -0.0493,  0.0251,  0.0509,\n",
       "                      -0.0507, -0.0649,  0.0378, -0.0783,  0.0190, -0.0575, -0.0501, -0.0491,\n",
       "                      -0.0151, -0.0201,  0.0229,  0.0669,  0.0360, -0.0183,  0.0348,  0.0567,\n",
       "                       0.0842, -0.0912,  0.0707,  0.0598, -0.0050,  0.0570, -0.0687, -0.0277,\n",
       "                      -0.0739, -0.0517,  0.0102,  0.0493, -0.0179,  0.0270, -0.0317, -0.0414,\n",
       "                      -0.0645, -0.0052,  0.0305,  0.0245, -0.0047,  0.0397, -0.0069,  0.0540,\n",
       "                      -0.0466,  0.0800, -0.0323, -0.0076, -0.0480,  0.0385, -0.0054, -0.0669,\n",
       "                       0.0145, -0.0839, -0.0703, -0.0591])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[ 0.0086, -0.0608, -0.1011, -0.0877,  0.1050,  0.0856,  0.0006,  0.0925,\n",
       "                        0.0508,  0.0346,  0.0015, -0.0787,  0.0846,  0.0779, -0.0096, -0.0746,\n",
       "                       -0.0703,  0.0007, -0.0070,  0.0557, -0.0710,  0.0274, -0.0540, -0.0713,\n",
       "                       -0.0753,  0.0509,  0.0889, -0.0398, -0.0046, -0.0526, -0.1035, -0.0659,\n",
       "                       -0.1038,  0.0753, -0.0272,  0.0570,  0.0877,  0.0439,  0.0878, -0.0666,\n",
       "                       -0.0243, -0.0200, -0.0865, -0.0568,  0.0827, -0.0280,  0.0655, -0.1009,\n",
       "                        0.0450,  0.0066, -0.0030,  0.0727, -0.0232,  0.1082, -0.0769,  0.0106,\n",
       "                       -0.0132,  0.0912,  0.0904,  0.1055, -0.0056,  0.0904,  0.0968, -0.0460,\n",
       "                       -0.0527,  0.0087,  0.0633, -0.0879, -0.0152,  0.0836,  0.0316, -0.0110,\n",
       "                        0.0837,  0.0144,  0.0312,  0.0484,  0.0347,  0.1026,  0.0948,  0.0975,\n",
       "                        0.0777, -0.0194, -0.0110, -0.0599],\n",
       "                      [-0.0747, -0.0196,  0.0422, -0.0407,  0.0012, -0.0198,  0.0786, -0.0293,\n",
       "                       -0.1063,  0.0575, -0.0974,  0.0242,  0.0122,  0.0193,  0.0086, -0.0918,\n",
       "                        0.0956,  0.0688,  0.0033,  0.0955, -0.0790, -0.0831, -0.0474, -0.0695,\n",
       "                        0.0267, -0.0201,  0.0455,  0.0719,  0.0430, -0.0267, -0.0761, -0.0245,\n",
       "                        0.0293, -0.0178,  0.0551, -0.0109, -0.0818,  0.0831,  0.0156, -0.1067,\n",
       "                       -0.0126,  0.0324,  0.0117,  0.0254, -0.0107,  0.0695,  0.0604,  0.0010,\n",
       "                        0.0086,  0.0378,  0.0391,  0.0657,  0.0539, -0.0090,  0.1061, -0.0236,\n",
       "                       -0.0953, -0.0437, -0.1021,  0.0169,  0.0010, -0.0175, -0.0583,  0.0569,\n",
       "                       -0.1064, -0.0249,  0.0197,  0.1038, -0.0315, -0.0564,  0.0275,  0.0809,\n",
       "                       -0.0327, -0.0088,  0.0675, -0.0232, -0.0968, -0.0929, -0.0229, -0.0483,\n",
       "                       -0.0785,  0.0268,  0.1019, -0.0805],\n",
       "                      [ 0.0639, -0.0308, -0.0259, -0.0442,  0.1088,  0.0894,  0.0126, -0.0986,\n",
       "                        0.0041,  0.0497,  0.0676, -0.0866, -0.0493, -0.0192, -0.0057, -0.0464,\n",
       "                       -0.0682, -0.1074,  0.0537, -0.0627, -0.0605,  0.0457, -0.0066,  0.0691,\n",
       "                       -0.0310, -0.0586, -0.0160,  0.0256, -0.0089, -0.0532,  0.0968,  0.0519,\n",
       "                        0.0702,  0.0511, -0.0517,  0.0865, -0.0301, -0.0456,  0.0159, -0.0287,\n",
       "                       -0.0256,  0.0448, -0.0400, -0.0613, -0.0188, -0.0700, -0.0651, -0.0994,\n",
       "                       -0.0401, -0.0642, -0.0347, -0.0576, -0.0485, -0.0281, -0.0040, -0.0468,\n",
       "                       -0.0352,  0.0286, -0.0908,  0.0730,  0.1009, -0.0664,  0.0526, -0.0392,\n",
       "                        0.0952,  0.0200,  0.0031, -0.0370, -0.0456, -0.0188, -0.0162,  0.0993,\n",
       "                       -0.0741,  0.0453,  0.0510,  0.0397,  0.0449, -0.0260, -0.0759, -0.0535,\n",
       "                        0.0141, -0.0110,  0.0214,  0.0541],\n",
       "                      [-0.0107, -0.0068, -0.0962, -0.0017, -0.0504, -0.0414, -0.0687,  0.0565,\n",
       "                        0.0586,  0.0148, -0.1037,  0.0022, -0.0678,  0.0145, -0.1002, -0.0824,\n",
       "                        0.0604,  0.0176,  0.0426, -0.0073,  0.1039, -0.0729,  0.0932,  0.0557,\n",
       "                       -0.0586, -0.1062, -0.0121,  0.0927,  0.0697,  0.0135,  0.0399, -0.0872,\n",
       "                       -0.1049,  0.0533,  0.1060, -0.0703, -0.0252,  0.0811,  0.0820,  0.0112,\n",
       "                        0.0351, -0.0673,  0.0390, -0.0897, -0.0788,  0.0733, -0.0502,  0.0594,\n",
       "                        0.0596, -0.1027,  0.0754,  0.0523,  0.1061, -0.0827,  0.0160, -0.0437,\n",
       "                        0.0137,  0.0625, -0.0921,  0.0841,  0.0946, -0.0990,  0.0576,  0.0405,\n",
       "                       -0.0443,  0.0556,  0.0091, -0.1001,  0.0337, -0.0782, -0.0272,  0.0555,\n",
       "                        0.0932, -0.0075, -0.0958,  0.0883, -0.0549, -0.0615,  0.0675, -0.1008,\n",
       "                       -0.0620,  0.0505, -0.0137, -0.0650],\n",
       "                      [-0.0965, -0.0399,  0.0663, -0.0424,  0.0458,  0.0420,  0.0331, -0.0377,\n",
       "                       -0.0164,  0.0763, -0.0978,  0.0627,  0.0696,  0.0144,  0.0936, -0.0163,\n",
       "                       -0.1076, -0.0082,  0.0369, -0.0463, -0.0983, -0.0830, -0.0718,  0.0329,\n",
       "                        0.0349, -0.1023,  0.0087,  0.0171, -0.0996, -0.0462, -0.0108, -0.1056,\n",
       "                       -0.0775,  0.0378,  0.0887,  0.0896, -0.1041, -0.0407,  0.0099,  0.0777,\n",
       "                        0.0398, -0.0180,  0.0425,  0.0463,  0.0470,  0.0911,  0.0294,  0.0231,\n",
       "                        0.0546,  0.0669,  0.0016,  0.0291,  0.0843,  0.0216,  0.0844,  0.0285,\n",
       "                       -0.0278, -0.0282, -0.0730,  0.0966,  0.1023, -0.0743, -0.0866, -0.0993,\n",
       "                       -0.1062,  0.0193,  0.0922, -0.0196,  0.1018,  0.0465, -0.0682,  0.0420,\n",
       "                        0.0612,  0.1039,  0.0914, -0.0692,  0.0378,  0.0005, -0.0401,  0.0245,\n",
       "                       -0.0472, -0.0844,  0.0293,  0.0416],\n",
       "                      [-0.0076, -0.0101, -0.0514,  0.0168,  0.1009,  0.0866, -0.0447, -0.0719,\n",
       "                        0.0801, -0.0023,  0.0770, -0.0849, -0.0025,  0.0672,  0.0174, -0.0059,\n",
       "                        0.0142, -0.1047,  0.0373, -0.0792, -0.0045, -0.1078,  0.0926, -0.0790,\n",
       "                        0.0927,  0.0462,  0.0375, -0.0622,  0.0434, -0.1016, -0.0983, -0.0085,\n",
       "                        0.0396, -0.0025, -0.0393, -0.0229,  0.0483,  0.0778,  0.0667, -0.0816,\n",
       "                       -0.0322, -0.0074,  0.0308, -0.0829, -0.0641, -0.0732, -0.1087, -0.0359,\n",
       "                       -0.0525,  0.0822,  0.0341,  0.1056,  0.0978, -0.0351, -0.0344, -0.0582,\n",
       "                       -0.0492, -0.0368, -0.0980,  0.0992,  0.0637, -0.0494,  0.0454,  0.0132,\n",
       "                       -0.0330,  0.0085,  0.0854, -0.0876, -0.0938,  0.0359,  0.0329, -0.0634,\n",
       "                       -0.0889,  0.1069,  0.0485, -0.0006,  0.0112,  0.0135, -0.0174, -0.0716,\n",
       "                        0.0604,  0.0375, -0.0921, -0.1040],\n",
       "                      [-0.0180,  0.0166, -0.0730,  0.0715, -0.0925,  0.0655, -0.0068, -0.0517,\n",
       "                        0.0117, -0.0970,  0.0482,  0.0220, -0.0444, -0.0653,  0.0253,  0.0021,\n",
       "                        0.1079,  0.0218, -0.0313, -0.0250, -0.0974,  0.0204,  0.0771,  0.1038,\n",
       "                       -0.0608, -0.0975, -0.0462,  0.0693, -0.0169,  0.1045,  0.0420,  0.0041,\n",
       "                       -0.0790,  0.0866, -0.1066,  0.0963,  0.0972, -0.0783, -0.0518, -0.0606,\n",
       "                        0.0830,  0.1024, -0.0290, -0.0309, -0.0382, -0.0517, -0.0580, -0.0544,\n",
       "                        0.0291, -0.0833, -0.1087,  0.0495,  0.0274,  0.0763,  0.0046, -0.0590,\n",
       "                       -0.0838,  0.1030,  0.1054, -0.0854, -0.1059,  0.0203,  0.0842,  0.0526,\n",
       "                        0.0337,  0.1015, -0.0025,  0.0632,  0.0235,  0.0334,  0.0016,  0.0126,\n",
       "                       -0.0278, -0.0193,  0.0535,  0.0537,  0.0530,  0.0943, -0.0884,  0.0236,\n",
       "                       -0.0626,  0.0683, -0.0784,  0.0969],\n",
       "                      [ 0.0114, -0.0668, -0.0934,  0.0698,  0.0939,  0.0791, -0.0441, -0.0125,\n",
       "                       -0.0270,  0.0576,  0.0288, -0.0612, -0.0273, -0.0015, -0.0395, -0.0041,\n",
       "                       -0.0118,  0.0972,  0.0980, -0.0786,  0.0706, -0.0496, -0.0901,  0.0002,\n",
       "                       -0.0083,  0.1042, -0.0014,  0.1082,  0.0668,  0.0131, -0.0972,  0.0527,\n",
       "                       -0.0608, -0.0444, -0.0998, -0.0245,  0.0637,  0.0659,  0.0790,  0.0644,\n",
       "                        0.0587,  0.0968, -0.0750,  0.0316,  0.0304, -0.0403, -0.0351,  0.0067,\n",
       "                        0.0913,  0.0406,  0.0884, -0.0932, -0.0312,  0.0404,  0.0654,  0.0279,\n",
       "                        0.0107,  0.1007,  0.0158, -0.0176,  0.1070, -0.0910,  0.0200,  0.0231,\n",
       "                        0.0912, -0.0491,  0.0389, -0.0450, -0.0997, -0.0903,  0.0146,  0.0083,\n",
       "                       -0.1055, -0.0791, -0.0263, -0.0189,  0.0123, -0.0810,  0.0834,  0.0840,\n",
       "                        0.0708, -0.0208,  0.0212, -0.0835],\n",
       "                      [ 0.0875, -0.0781, -0.0408, -0.0423,  0.1082,  0.0048,  0.0114,  0.0845,\n",
       "                       -0.0967, -0.0848,  0.0759,  0.0682,  0.0540, -0.0595,  0.0269, -0.0896,\n",
       "                        0.0778, -0.0748,  0.0604,  0.1079, -0.0968, -0.0537, -0.0428, -0.0257,\n",
       "                        0.0206,  0.0268,  0.0286,  0.0647,  0.0843, -0.0371,  0.0070,  0.0663,\n",
       "                        0.0957,  0.0143, -0.0576,  0.1010,  0.0254,  0.0172,  0.0678, -0.0497,\n",
       "                        0.0034, -0.0108,  0.0549,  0.0037, -0.0621,  0.0267,  0.0589,  0.0286,\n",
       "                        0.0801,  0.0415, -0.0321,  0.0244, -0.0578, -0.0476,  0.0724,  0.0691,\n",
       "                       -0.0397, -0.0157,  0.0293, -0.0218, -0.0342, -0.0399,  0.0344, -0.0342,\n",
       "                       -0.0803,  0.0015,  0.0914, -0.0648, -0.0747,  0.0705, -0.0376, -0.0920,\n",
       "                        0.0658, -0.0842,  0.0886,  0.0127, -0.0553, -0.0651, -0.0012,  0.0388,\n",
       "                       -0.0123, -0.0128, -0.0428, -0.0475],\n",
       "                      [-0.0445, -0.0547,  0.0813,  0.0483,  0.0330, -0.0044,  0.0097,  0.0860,\n",
       "                        0.0632, -0.0481, -0.0905,  0.0325, -0.0612,  0.0860,  0.0788, -0.0350,\n",
       "                        0.0885,  0.0638, -0.0211, -0.0243, -0.0888, -0.0617,  0.0247, -0.0303,\n",
       "                        0.0626,  0.0443, -0.0002, -0.0560,  0.0135, -0.0707,  0.0988, -0.0854,\n",
       "                        0.0132,  0.0813,  0.0396, -0.0948,  0.0849, -0.1006,  0.0491,  0.0017,\n",
       "                       -0.0933,  0.0251, -0.0372, -0.0771,  0.0515, -0.0688,  0.0207, -0.0118,\n",
       "                       -0.0278,  0.0488,  0.0899, -0.0480,  0.1007, -0.0006,  0.0974,  0.0062,\n",
       "                       -0.0949,  0.0026, -0.0082, -0.0514,  0.0387,  0.0087, -0.0025,  0.0249,\n",
       "                       -0.0904,  0.0420, -0.0237, -0.0991, -0.0799, -0.0360, -0.0096, -0.0128,\n",
       "                        0.0173, -0.0130,  0.0292,  0.0579,  0.0757,  0.0571,  0.0349,  0.0096,\n",
       "                       -0.0038, -0.0927, -0.0291, -0.0131]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([-0.0664, -0.0652, -0.0501,  0.0613, -0.0989,  0.0752,  0.0710, -0.0985,\n",
       "                       0.0311, -0.0923]))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e34b50d2-cd48-4c27-b7ee-0a91420ec8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b9be1646-8019-42f9-8ab5-acea9b59a3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241.0390625"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perBytes = 4\n",
    "(6*1*5*5+16*6*5*5+120*400+84*120+10*84+6+16+120+84+10)*perBytes/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "afc86195-e6b5-434d-a6e3-22c6707694b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(an.state_dict(), \"./net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f179a241-5ab4-4bd8-bc2c-39caffed0975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0333, grad_fn=<SelectBackward0>)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p[0][0][0][0])\n",
    "    print(p[0][0][0][0].dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3db847f3-ae0a-4dfb-a985-6c6f4e7a2188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0333,  0.0651, -0.0805, -0.0726, -0.0010],\n",
      "          [-0.0628, -0.1703,  0.1084, -0.0266,  0.0225],\n",
      "          [-0.0297,  0.0236,  0.1508,  0.0582,  0.0581],\n",
      "          [ 0.1026,  0.1341,  0.0807,  0.1285,  0.1105],\n",
      "          [-0.1772,  0.1706,  0.1381,  0.1293,  0.0070]]],\n",
      "\n",
      "\n",
      "        [[[-0.0370,  0.1529,  0.0437,  0.0769, -0.0402],\n",
      "          [-0.1830, -0.1313,  0.1884, -0.0552,  0.0977],\n",
      "          [-0.1966,  0.0685, -0.1407, -0.1272, -0.0549],\n",
      "          [-0.0218,  0.1325, -0.1874,  0.0541, -0.1716],\n",
      "          [-0.1121, -0.0900, -0.1455, -0.0328, -0.0427]]],\n",
      "\n",
      "\n",
      "        [[[-0.1476,  0.0029, -0.1518,  0.0579, -0.0667],\n",
      "          [ 0.0224,  0.1465,  0.1510, -0.1387, -0.0958],\n",
      "          [ 0.0939, -0.0736,  0.1566, -0.1097,  0.0024],\n",
      "          [-0.1572, -0.1649, -0.1314,  0.1645, -0.0210],\n",
      "          [-0.0298, -0.1167, -0.1056,  0.1011, -0.1954]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0453,  0.0064, -0.1390,  0.1950,  0.1454],\n",
      "          [-0.1826, -0.1646,  0.0784, -0.1364, -0.1568],\n",
      "          [-0.0253, -0.1463,  0.1325, -0.1834, -0.0823],\n",
      "          [-0.1256,  0.0544,  0.0322, -0.0752,  0.1992],\n",
      "          [ 0.0649,  0.0701, -0.1134, -0.0317, -0.0162]]],\n",
      "\n",
      "\n",
      "        [[[-0.0486, -0.1440,  0.0940, -0.0674,  0.0256],\n",
      "          [-0.1099,  0.0252,  0.1932, -0.0849, -0.0437],\n",
      "          [ 0.1399,  0.0705,  0.1881,  0.0942, -0.1741],\n",
      "          [ 0.1592,  0.0360, -0.0441, -0.0882,  0.1692],\n",
      "          [ 0.0639, -0.0983, -0.1971,  0.1018,  0.1907]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1046, -0.1944,  0.0744, -0.0369,  0.1084],\n",
      "          [-0.1616,  0.1197,  0.1039, -0.0384,  0.0147],\n",
      "          [-0.0351, -0.1806,  0.1749,  0.0280,  0.1320],\n",
      "          [-0.0782, -0.0353, -0.0605,  0.1054, -0.1182],\n",
      "          [-0.1939, -0.1617,  0.1563, -0.1904, -0.0451]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56075fb6-37b8-4092-ac6e-7754205bc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)       # Current CPU\n",
    "    torch.cuda.manual_seed(seed)  # Current GPU\n",
    "    np.random.seed(seed)          # Numpy module\n",
    "    random.seed(seed)             # Python random module\n",
    "    torch.backends.cudnn.benchmark = False    # Close optimization\n",
    "    torch.backends.cudnn.deterministic = True # Close optimization\n",
    "    torch.cuda.manual_seed_all(seed) # All GPU (Optional)\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c3c6c-aa8f-4a25-946b-c454636a7d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed90669-2e45-4e4a-9a44-617331a24031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b888c901-5f10-4ff1-a0b2-50ce744cd08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/80], Step [100/500] Loss: 1.6119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [114]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/code/mygithub/AIWork/models/ResNet.py:65\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m     64\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(out)\n\u001b[0;32m---> 65\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(out)\n\u001b[1;32m     67\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_pool(out)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/code/mygithub/AIWork/models/ResNet.py:22\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     21\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 22\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146028e9-a495-4a3c-ae7b-6da451b0046d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
